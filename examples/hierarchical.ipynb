{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730e5b6d",
   "metadata": {},
   "source": [
    "### Hierarchical classification training demo\n",
    "Based on my previous work.\n",
    "TODO: Switch to Guillaume's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urljoin\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from hierarchical.utils import create_hierarchy, mask_hierarchy\n",
    "from hierarchical.loss import MultiLevelCrossEntropyLoss\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "from tqdm.notebook import tqdm as TQDM\n",
    "\n",
    "from mini_trainer.classifier import get_model\n",
    "from mini_trainer.utils import ImageClassLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d324e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"https://anon.erda.au.dk/share_redirect/btK3WIh4Md/rebalanced75_without_larvae/\"\n",
    "\n",
    "with NamedTemporaryFile(suffix = \"txt\") as tmpfile:\n",
    "    urlretrieve(\n",
    "        urljoin(repo, \"folder_index.txt\"),\n",
    "        tmpfile.name\n",
    "    )\n",
    "    with open(tmpfile.name, \"r\") as f:\n",
    "        file_index = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23a40c",
   "metadata": {},
   "source": [
    "### Extract the class hierarchy from the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8879c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16039bee40e54ebd8b4e9dbfb7186016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding unique classes...:   0%|          | 0/1405516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hierarchy = sorted(filter(lambda x : len(x) == 3, set(tuple(path.split(\"/\")[:-1]) for path in TQDM(file_index, desc=\"Finding unique classes...\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d633d2",
   "metadata": {},
   "source": [
    "Count the number of images per class (species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da56183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28284e76062744a097b3adca33168133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting images per class...:   0%|          | 0/1405516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = defaultdict(lambda : 0)\n",
    "\n",
    "for path in TQDM(file_index, desc=\"Counting images per class...\"):\n",
    "    path_from_root = tuple(path.split(\"/\")[:-1])\n",
    "    if path_from_root in hierarchy:\n",
    "        counts[path_from_root] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b1b0c",
   "metadata": {},
   "source": [
    "Filter species with less than 1000 images, and select 200 training images and 25 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfae3381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes with more than 1000 images: 420\n",
      "Number of total images for selected classes: 1174143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb01243632034f4ea1f128ca3df9161a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Allocating train/test images...:   0%|          | 0/1405516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 84000\n",
      "Number of test images: 10500\n"
     ]
    }
   ],
   "source": [
    "more_than_1000 = set(cls for cls in hierarchy if counts[cls] >= 1000)\n",
    "print(f'Number of classes with more than 1000 images: {len(more_than_1000)}')\n",
    "print(f'Number of total images for selected classes: {sum(counts[cls] for cls in more_than_1000)}')\n",
    "train_counts, test_counts = defaultdict(lambda : 0), defaultdict(lambda : 0)\n",
    "train_images, test_images = [], []\n",
    "for path in TQDM(file_index, desc=\"Allocating train/test images...\"):\n",
    "    path_from_root = tuple(path.split(\"/\")[:-1])\n",
    "    if not (path_from_root in more_than_1000):\n",
    "        continue\n",
    "    if train_counts[path_from_root] < 200:\n",
    "        train_counts[path_from_root] += 1\n",
    "        train_images.append(path)\n",
    "    elif test_counts[path_from_root] < 25:\n",
    "        test_counts[path_from_root] += 1\n",
    "        test_images.append(path)\n",
    "print(f'Number of training images: {len(train_images)}')\n",
    "print(f'Number of test images: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016273bb",
   "metadata": {},
   "source": [
    "### Download train/test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8bc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d410b6980ac4081ad26e1f63deec913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading test images...:   0%|          | 0/10500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bd86b12eb34be1ad3c76cec958af61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train images...:   0%|          | 0/84000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dir = os.path.join(\"hierarchical\", \"train\")\n",
    "test_dir = os.path.join(\"hierarchical\", \"test\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def _download_one(f, d, override=False):\n",
    "    lpath = os.path.join(d, f)\n",
    "    if os.path.exists(lpath):\n",
    "        if override:\n",
    "            os.remove(lpath)\n",
    "        else:\n",
    "            return\n",
    "    os.makedirs(os.path.dirname(lpath), exist_ok=True)\n",
    "    urlretrieve(\n",
    "        urljoin(repo, f),\n",
    "        lpath\n",
    "    )\n",
    "\n",
    "thread_map(lambda x : _download_one(x, test_dir), test_images, tqdm_class=TQDM, desc=\"Downloading test images...\")\n",
    "thread_map(lambda x : _download_one(x, train_dir), train_images, tqdm_class=TQDM, desc=\"Downloading train images...\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca53d29",
   "metadata": {},
   "source": [
    "### Define custom hierarchical architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24299ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, in_features : int, out_features : int, masks : Optional[List[torch.Tensor]]=None, hidden : bool=False):\n",
    "        super().__init__()\n",
    "        # Create a BatchNormalization Layer\n",
    "        self.batch_norm = nn.BatchNorm1d(in_features)\n",
    "\n",
    "        # Create one hidden layer\n",
    "        self.hidden = hidden and nn.Linear(in_features, in_features)\n",
    "\n",
    "        # Create a standard linear layer.\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "        # Store masks\n",
    "        self.masks = [m.T.clone(memory_format=torch.contiguous_format) for m in masks] or []\n",
    "        [m.requires_grad_(False) for m in self.masks]\n",
    "        \n",
    "        # Set the bias to -1 and freeze it.\n",
    "        with torch.no_grad():\n",
    "            self.linear.bias.fill_(-1)\n",
    "        self.linear.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hidden:\n",
    "            x = nn.functional.leaky_relu(self.hidden(x), True)\n",
    "        x = self.batch_norm(x)\n",
    "        # Compute the normalized log probabilities for the leaf nodes (level 0)\n",
    "        y0 = nn.functional.log_softmax(self.linear(x), dim = 1)\n",
    "        ys = [y0]\n",
    "        # Propagate the probabilities up the hierarchy using the masks\n",
    "        for mask in self.masks:\n",
    "            ys.append(torch.logsumexp(ys[-1].unsqueeze(2) + mask, dim = 1))\n",
    "        return ys\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(model_type : str, path : str, masks : Optional[List[torch.Tensor]]=None, device=torch.device(\"cpu\"), dtype=torch.float32):\n",
    "        # Parse model architecture\n",
    "        architecture, head_name, _ = get_model(model_type)\n",
    "\n",
    "        # Read weight file\n",
    "        weights = torch.load(path, device, weights_only=True)\n",
    "        num_classes, num_embeddings = weights[f\"{head_name}.linear.weight\"].shape\n",
    "        \n",
    "        # Load weights into model architecture\n",
    "        setattr(architecture, head_name, HierarchicalClassifier(num_embeddings, num_classes, masks.to(device, dtype)))\n",
    "        architecture.load_state_dict(weights)\n",
    "        architecture.to(device, dtype)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "def hierarchical_model_builder(\n",
    "        model : str,\n",
    "        masks : List[torch.Tensor],\n",
    "        weights : Optional[str],\n",
    "        fine_tune : bool,\n",
    "        device : torch.device,\n",
    "        dtype : torch.dtype,\n",
    "        num_classes : int,\n",
    "        **kwargs : Any\n",
    "    ) -> Tuple[nn.Module, Callable[[torch.Tensor], torch.Tensor]]:\n",
    "    if len(kwargs) != 0:\n",
    "        unexpected = \", \".join(kwargs.keys())\n",
    "        raise TypeError(f\"my_fun() got unexpected keyword argument(s): {unexpected}\")\n",
    "    model, head_name, model_preprocess = get_model(model)\n",
    "    model : nn.Module\n",
    "    if not isinstance(model, nn.Module):\n",
    "        raise TypeError(f\"Unknown model type `{type(model)}`, expected `{nn.Module}`\")\n",
    "    num_embeddings = getattr(model, head_name)[1].in_features\n",
    "    if weights is not None:\n",
    "        model = HierarchicalClassifier.load(\n",
    "            model_type=model, \n",
    "            path=weights, \n",
    "            masks=[m.to(device, torch.float32) for m in masks], \n",
    "            device=device, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    else:\n",
    "        classifier = HierarchicalClassifier(\n",
    "            in_features=num_embeddings, \n",
    "            out_features=num_classes, \n",
    "            masks=[m.to(device, torch.float32) for m in masks]\n",
    "        )\n",
    "        setattr(model, head_name, classifier)\n",
    "        model.to(device, torch.float32)\n",
    "    if fine_tune:\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and not head_name in name:\n",
    "                param.requires_grad_(False)\n",
    "    return model, model_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac62017",
   "metadata": {},
   "source": [
    "### Define utilities for overriding the default class handling behaviour in mini_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337c3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_parse_class_index(path : Optional[str]=None, dir : Optional[str]=None):\n",
    "    \"\"\"\n",
    "    Accepts a path to a class index file or a directory with named subdirectories for each class.\n",
    "    \"\"\"\n",
    "    if path is None or not os.path.exists(path):\n",
    "        if dir is None or not os.path.isdir(dir):\n",
    "            raise TypeError(f'If `path` is not the path to a valid file, `dir` must be a valid directory, not \\'{dir}\\'.')\n",
    "        combinations = sorted(set(tuple(f.split(os.sep)[:-1]) for f in glob.glob(\"**\", root_dir=dir, recursive=True) if not os.path.isdir(os.path.join(dir, f))))\n",
    "        classes = {i : [] for i in range(len(next(iter(combinations))))}\n",
    "        for e in combinations:\n",
    "            for lvl, cls in enumerate(e):\n",
    "                if cls not in classes[lvl]:\n",
    "                    classes[lvl].append(cls)\n",
    "        classes = {lvl : classes[lvl] for lvl in range(len(classes))}\n",
    "        cls2idx = {lvl : {cls : idx for idx, cls in enumerate(classes[lvl])} for lvl in range(len(classes))}\n",
    "        if path is not None:\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump({\"cls2idx\" : cls2idx, \"combinations\" : combinations}, f)\n",
    "    else:\n",
    "        with open(path, \"rb\") as f:\n",
    "            c2i_comb = json.load(f)\n",
    "            cls2idx = {int(lvl) : e for lvl, e in c2i_comb[\"cls2idx\"].items()}\n",
    "            combinations = c2i_comb[\"combinations\"]\n",
    "    cls = [list(cls2idx[lvl]) for lvl in range(len(cls2idx))]\n",
    "    idx2cls = {lvl : {v : k for k, v in cls2idx[lvl].items()} for lvl in range(len(cls2idx))}\n",
    "    ncls = [len(clvl) for clvl in cls]\n",
    "    hierarchy = create_hierarchy(combinations, cls2idx)\n",
    "    masks = mask_hierarchy(hierarchy, zero=-100)\n",
    "    return {\"num_classes\" : ncls[-1], \"masks\" : list(reversed(masks))}, {\"classes\" : cls, \"class2idx\" : cls2idx, \"idx2class\" : idx2cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_dataloader_builder(\n",
    "        data_index : str,\n",
    "        input_dir : str,\n",
    "        classes : List[str],\n",
    "        class2idx : Dict[str, int],\n",
    "        preprocess : Callable[[torch.Tensor], torch.Tensor],\n",
    "        batch_size : int,\n",
    "        device : torch.device,\n",
    "        dtype = torch.dtype,\n",
    "        resize_size : Optional[int]=None,\n",
    "        train_proportion : float=0.9,\n",
    "        idx2class : Optional[Dict[int, str]]=None\n",
    "    ):\n",
    "    # Prepare datasets/dataloaders\n",
    "    if data_index is None:\n",
    "        all_files = [path for f in glob.glob(\"**\", root_dir=input_dir, recursive=True) if not os.path.isdir(path := os.path.join(input_dir, f))]\n",
    "        data = {\n",
    "            \"path\" : [],\n",
    "            \"class\" : [],\n",
    "            \"split\" : []\n",
    "        }\n",
    "        for path in all_files:\n",
    "            data[\"path\"].append(path)\n",
    "            data[\"class\"].append([class2idx[lvl][cls] for lvl, cls in enumerate(os.path.relpath(path, input_dir).split(os.sep)[:-1])])\n",
    "            data[\"split\"].append(\"train\" if random.random() < train_proportion else \"validation\")\n",
    "        data = {k : np.array(v) for k, v in data.items()}\n",
    "        train_image_data = {k : v[data[\"split\"] == np.array(\"train\")] for k, v in data.items()}\n",
    "        val_image_data = {k : v[data[\"split\"] == np.array(\"validation\")] for k, v in data.items()}\n",
    "        \n",
    "    resize_size = preprocess.resize_size if hasattr(preprocess, \"resize_size\") else resize_size\n",
    "\n",
    "    if not isinstance(resize_size, int):\n",
    "        if not (isinstance(resize_size, tuple) and len(resize_size) == 2 and all(map(lambda x : isinstance(x, int), resize_size))):\n",
    "            raise TypeError(f'Invalid resize size passed, foun {resize_size}, but expected an integer or a tuple of two integers')\n",
    "    print(f\"Building datasets with image size {resize_size}\")\n",
    "\n",
    "    def path2cls2idx(path, cls2idx=class2idx, nlvl=len(class2idx)):\n",
    "        return torch.tensor(list(reversed([cls2idx[lvl][cls] for lvl, cls in enumerate(path.split(os.sep)[:-1][-nlvl:])]))).long().to(device)\n",
    "\n",
    "    loader = ImageClassLoader(path2cls2idx, preprocess, dtype, device)\n",
    "    train_dataset = loader(train_image_data[\"path\"])\n",
    "    val_dataset = loader(val_image_data[\"path\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    val_sampler = SequentialSampler(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=val_sampler, \n",
    "        num_workers=0, \n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fc4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8526088..2.199462].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets with image size 256\n",
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m         targets = targets.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m MultiLevelLoss([(\u001b[38;5;28mself\u001b[39m._loss_fns[i](preds[i], targets[i])).mean() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_levels)], \u001b[38;5;28mself\u001b[39m.weights).aggregate()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhierarchical/train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mefficientnet_b0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspec_model_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhierarchical_parse_class_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_builder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhierarchical_dataloader_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_builder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhierarchical_model_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion_builder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMultiLevelCrossEntropyLoss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel_smoothing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweights\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.65\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/train.py:335\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(input, output, model, checkpoint, weights, data_index, class_index, fine_tune, learning_rate, epochs, batch_size, warmup_epochs, name, device, dtype, seed, spec_model_dataloader, spec_model_dataloader_kwargs, model_builder, model_builder_kwargs, dataloader_builder, dataloader_builder_kwargs, augmentation_builder, augmentation_builder_kwargs, criterion_builder, criterion_kwargs, optimizer_builder, optimizer_kwargs, lr_schedule_builder, lr_schedule_builder_kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstart_epoch\u001b[39m\u001b[33m'\u001b[39m\u001b[33m value in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` but expected an `int`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_preprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Save result model\u001b[39;00m\n\u001b[32m    352\u001b[39m nn_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/trainer.py:143\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, test_loader, criterion, optimizer, lr_scheduler, epochs, start_epoch, preprocess, augmentation, device, dtype, output_dir, **kwargs)\u001b[39m\n\u001b[32m    141\u001b[39m start_time = time.time()\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, epochs):\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     evaluate(model, criterion, test_loader, preprocess, device=device, dtype=dtype)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_dir:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/trainer.py:44\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, criterion, optimizer, lr_scheduler, data_loader, epoch, preprocess, augmentation, clip_grad_norm, device, dtype)\u001b[39m\n\u001b[32m     40\u001b[39m metric_logger.add_meter(\u001b[33m\"\u001b[39m\u001b[33mimg/s\u001b[39m\u001b[33m\"\u001b[39m, SmoothedValue(window_size=\u001b[32m10\u001b[39m, fmt=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{value}\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     42\u001b[39m header = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetric_logger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/utils.py:396\u001b[39m, in \u001b[36mMetricLogger.log_every\u001b[39m\u001b[34m(self, iterable, print_freq, header)\u001b[39m\n\u001b[32m    392\u001b[39m     log_msg = \u001b[38;5;28mself\u001b[39m.delimiter.join(\n\u001b[32m    393\u001b[39m         [header, \u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m{\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m + space_fmt + \u001b[33m\"\u001b[39m\u001b[33m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtime: \u001b[39m\u001b[38;5;132;01m{time}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[33m\"\u001b[39m]\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    395\u001b[39m MB = \u001b[32m1024.0\u001b[39m * \u001b[32m1024.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/utils.py:103\u001b[39m, in \u001b[36mLazyDataset.__getitem__\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mini_trainer/mini_trainer/utils.py:150\u001b[39m, in \u001b[36mImageClassLoader.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x : Union[\u001b[38;5;28mstr\u001b[39m, Iterable]):\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         proc_img : torch.Tensor = \u001b[38;5;28mself\u001b[39m.preprocessor(resize(\u001b[38;5;28mself\u001b[39m.converter(\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mImageReadMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRGB\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;28mself\u001b[39m.shape)).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mcls\u001b[39m = \u001b[38;5;28mself\u001b[39m.class_decoder(x)\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m proc_img, \u001b[38;5;28mcls\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/torchvision/io/image.py:324\u001b[39m, in \u001b[36mdecode_image\u001b[39m\u001b[34m(input, mode, apply_exif_orientation)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    323\u001b[39m     mode = ImageReadMode[mode.upper()]\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_exif_orientation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/mini_trainer/lib/python3.12/site-packages/torch/_ops.py:1123\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from mini_trainer.train import main\n",
    "\n",
    "main(\n",
    "    input=\"hierarchical/train\",\n",
    "    # model=\"efficientnet_b0\",\n",
    "    spec_model_dataloader=hierarchical_parse_class_index,\n",
    "    dataloader_builder=hierarchical_dataloader_builder,\n",
    "    model_builder=hierarchical_model_builder,\n",
    "    criterion_builder=MultiLevelCrossEntropyLoss,\n",
    "    criterion_kwargs={\"label_smoothing\" : 0, \"weights\" : [0.1, 0.25, 0.65]}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini_trainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
